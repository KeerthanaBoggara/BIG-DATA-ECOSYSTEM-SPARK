{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "JJKh7_xu1ZnF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create SparkConf and SparkContext\n",
        "conf = SparkConf().setAppName(\"projectName\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf)"
      ],
      "metadata": {
        "id": "xOe-iUBG1Yq_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
      ],
      "metadata": {
        "id": "pVSLxN9X1Xkj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HGW4Nb4E0_u1"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    {\"Name\": \"Alice\", \"Age\": 25, \"City\": \"New York\"},\n",
        "    {\"Name\": \"Bob\", \"Age\": 30, \"City\": \"San Francisco\"},\n",
        "    {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Los Angeles\"},\n",
        "    {\"Name\": \"David\", \"Age\": 40, \"City\": \"Chicago\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName(\"projectName\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf)"
      ],
      "metadata": {
        "id": "-Ul_OSeZ1EJf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(data)\n",
        "type(rdd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "BlzzJI5F1Kji",
        "outputId": "38169ce5-0aa8-47e3-ca0c-0450faee3f10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.rdd.RDD</b><br/>def __init__(jrdd: &#x27;JavaObject&#x27;, ctx: &#x27;SparkContext&#x27;, jrdd_deserializer: Serializer=AutoBatchedSerializer(CPickleSerializer()))</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py</a>A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
              "Represents an immutable, partitioned collection of elements that can be\n",
              "operated on in parallel.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 336);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = rdd.toDF()\n",
        "type(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "2J8_dj3j1NHx",
        "outputId": "4c2b082b-aed0-4b68-f444-0b5ee042cd69"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the DataFrame to a parquet file, overwriting if the file exists.\n",
        "file_path = \"my_data.parquet\"\n",
        "df.write.format(\"parquet\").mode(\"overwrite\").save(file_path)"
      ],
      "metadata": {
        "id": "vO83cDHR1j5u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the parquet file back into a DataFrame.\n",
        "df_loaded = spark.read.format(\"parquet\").load(file_path)\n",
        "df_loaded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Xrjb6p17_-",
        "outputId": "64b704c4-71c0-4d55-8062-5ce4428d6bce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+-------+\n",
            "|Age|         City|   Name|\n",
            "+---+-------------+-------+\n",
            "| 35|  Los Angeles|Charlie|\n",
            "| 40|      Chicago|  David|\n",
            "| 25|     New York|  Alice|\n",
            "| 30|San Francisco|    Bob|\n",
            "+---+-------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define data\n",
        "[\n",
        "    {\"Name\": \"Alice\", \"Age\": 25, \"City\": \"New York\"},\n",
        "    {\"Name\": \"Bob\", \"Age\": 30, \"City\": \"San Francisco\"},\n",
        "    {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Los Angeles\"},\n",
        "    {\"Name\": \"David\", \"Age\": 40, \"City\": \"Chicago\"}\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV file\n",
        "df.to_csv(\"data.csv\", index=False)\n",
        "\n",
        "print(\"CSV file created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdwM9dWn3-rk",
        "outputId": "e5616597-e474-4d8d-b91d-82982b7b03b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('data.csv')"
      ],
      "metadata": {
        "id": "kPDr6sTB2EDT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data for multiple files\n",
        "data1 = {\"Name\": [\"Alice\", \"Bob\"], \"Age\": [25, 30], \"Salary\": [50000, 60000]}\n",
        "data2 = {\"Name\": [\"Charlie\", \"David\"], \"Age\": [35, 40], \"Salary\": [70000, 80000]}\n",
        "data3 = {\"Name\": [\"Eve\", \"Frank\"], \"Age\": [45, 50], \"Salary\": [90000, 100000]}\n",
        "\n",
        "# Convert to DataFrames\n",
        "df1 = pd.DataFrame(data1)\n",
        "df2 = pd.DataFrame(data2)\n",
        "df3 = pd.DataFrame(data3)\n",
        "\n",
        "# Save DataFrames as CSV files\n",
        "df1.to_csv(\"file1.csv\", index=False)\n",
        "df2.to_csv(\"file2.csv\", index=False)\n",
        "df3.to_csv(\"file3.csv\", index=False)\n",
        "\n",
        "print(\"CSV files created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOi52UrL4r8M",
        "outputId": "d5e65d4b-f23c-407f-f01e-62fcc78fab83"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV files created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"Read_Multiple_CSVs\").getOrCreate()\n",
        "\n",
        "# List of CSV file paths\n",
        "csv_files = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\n",
        "\n",
        "# Read all files into a single DataFrame\n",
        "df = spark.read.csv(csv_files, header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raz3v0Cu4aNM",
        "outputId": "68b41d33-825a-4b11-c172-9e783fb66596"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   Name|Age|Salary|\n",
            "+-------+---+------+\n",
            "|Charlie| 35| 70000|\n",
            "|  David| 40| 80000|\n",
            "|    Eve| 45| 90000|\n",
            "|  Frank| 50|100000|\n",
            "|  Alice| 25| 50000|\n",
            "|    Bob| 30| 60000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read multiple CSV files while keeping headers\n",
        "df = spark.read.option(\"header\", \"true\").csv([\"file1.csv\", \"file2.csv\", \"file3.csv\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8GvKVNF5Aoi",
        "outputId": "f6e5d7fd-edd0-42e2-fb0b-dc2d6f60b8d5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   Name|Age|Salary|\n",
            "+-------+---+------+\n",
            "|Charlie| 35| 70000|\n",
            "|  David| 40| 80000|\n",
            "|    Eve| 45| 90000|\n",
            "|  Frank| 50|100000|\n",
            "|  Alice| 25| 50000|\n",
            "|    Bob| 30| 60000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(header=True, inferSchema=True).csv([\"file1.csv\", \"file2.csv\", \"file3.csv\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ2VowiY46xS",
        "outputId": "86b23c46-7d05-4865-98d7-da1a5232f8b8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   Name|Age|Salary|\n",
            "+-------+---+------+\n",
            "|Charlie| 35| 70000|\n",
            "|  David| 40| 80000|\n",
            "|    Eve| 45| 90000|\n",
            "|  Frank| 50|100000|\n",
            "|  Alice| 25| 50000|\n",
            "|    Bob| 30| 60000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(header=True).csv([\"file1.csv\", \"file2.csv\", \"file3.csv\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m4_2uwm5mvL",
        "outputId": "1d1f1c6b-5c06-4de8-a6d1-e7b807abedbf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   Name|Age|Salary|\n",
            "+-------+---+------+\n",
            "|Charlie| 35| 70000|\n",
            "|  David| 40| 80000|\n",
            "|    Eve| 45| 90000|\n",
            "|  Frank| 50|100000|\n",
            "|  Alice| 25| 50000|\n",
            "|    Bob| 30| 60000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy text file for demonstration\n",
        "file_path = \"dataspark.txt\"\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(\"Alice 34 Bob 45 Charlie 30.\")\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "df = spark.read.text(file_path)\n",
        "\n",
        "# Show the DataFrame content\n",
        "df.show(truncate=False)\n",
        "df = spark.read.text(file_path)\n",
        "\n",
        "# Show the DataFrame content\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaNSJ5rQ6nHp",
        "outputId": "801e8e65-aa1e-46c2-9dda-78d737a90e1b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+\n",
            "|value                      |\n",
            "+---------------------------+\n",
            "|Alice 34 Bob 45 Charlie 30.|\n",
            "+---------------------------+\n",
            "\n",
            "+---------------------------+\n",
            "|value                      |\n",
            "+---------------------------+\n",
            "|Alice 34 Bob 45 Charlie 30.|\n",
            "+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"CSV from Text File\").getOrCreate()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpzYxJUr_Xlp",
        "outputId": "7dd81705-d02a-4354-f03e-a0461d607d1b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|Alice 34 Bob 45 C...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Dummy data to be written to the JSON file\n",
        "dummy_data = [\n",
        "    [\n",
        "    {\n",
        "        \"name\": \"Alice\",\n",
        "        \"age\": 34\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Bob\",\n",
        "        \"age\": 45\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Charlie\",\n",
        "        \"age\": 30\n",
        "    }\n",
        "]\n",
        "]\n",
        "\n",
        "# Write the dummy data to a JSON file\n",
        "with open(\"dummy_data.json\", \"w\") as json_file:\n",
        "    json.dump(dummy_data, json_file, indent=4)\n",
        "\n",
        "print(\"Dummy JSON file created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_BEUOJO_2Wm",
        "outputId": "9ab5f613-d5e7-4983-95cd-a638eed63b17"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy JSON file created successfully!\n"
          ]
        }
      ]
    }
  ]
}